{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b4c17a",
   "metadata": {},
   "source": [
    "### Load the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2924bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the methods/processes\n",
    "methods = ['default_essay','self_consistency_essay_n3', 'self_consistency_essay_n5', 'self_discover_essay']\n",
    "\n",
    "# Create a dictionary to store all results\n",
    "results_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f42c2e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded grading details successfully: 43 questions\n",
      "üìä Total possible points across all questions: 149\n",
      "üìà Max scores per question: [6, 2, 2, 5, 2, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 3, 3, 5, 5, 4, 3, 7, 3, 4, 4, 4, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "# Load the grading details file and extract max scores in order\n",
    "# We assume the questions are in the same order in both files\n",
    "try:\n",
    "    with open('data/answer_grading_details.json', 'r') as f:\n",
    "        grading_details_list = json.load(f)\n",
    "    \n",
    "    # Extract max scores in order - assuming same order as evaluated results\n",
    "    max_scores_per_question = [item['max_score'] for item in grading_details_list]\n",
    "    total_possible_points = sum(max_scores_per_question)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded grading details successfully: {len(max_scores_per_question)} questions\")\n",
    "    print(f\"üìä Total possible points across all questions: {total_possible_points}\")\n",
    "    print(f\"üìà Max scores per question: {max_scores_per_question}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading grading details: {e}\")\n",
    "    max_scores_per_question = []\n",
    "    total_possible_points = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "75a9984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_file(filepath):\n",
    "    \"\"\"\n",
    "    Process a single JSON file and calculate the aggregate score using actual max scores\n",
    "    Assumes questions are in the same order as the grading details file\n",
    "    Returns: (model_name, total_score, num_questions, normalized_score)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract model name from filename\n",
    "        filename = os.path.basename(filepath)\n",
    "        model_name = filename.replace('evaluated_results_', '').replace('.json', '').split('__')[0]\n",
    "        \n",
    "        # Calculate total self_grade_score\n",
    "        total_score = 0\n",
    "        num_questions_with_scores = 0\n",
    "        \n",
    "        for idx, item in enumerate(data):\n",
    "            if 'self_grade_score' in item and item['self_grade_score'] is not None:\n",
    "                total_score += item['self_grade_score']\n",
    "                num_questions_with_scores += 1\n",
    "        \n",
    "        # Calculate normalized score using the predetermined total possible points\n",
    "        # We assume all questions should have scores, so we use the full total_possible_points\n",
    "        normalized_score = total_score / total_possible_points if total_possible_points > 0 else 0\n",
    "        \n",
    "        return model_name, total_score, num_questions_with_scores, normalized_score\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filepath}: {e}\")\n",
    "        return None, 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9a5aa5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for invalid scores (> max_score or < 0)...\n",
      "============================================================\n",
      "‚úÖ No invalid scores found. All scores are within valid ranges!\n"
     ]
    }
   ],
   "source": [
    "def check_invalid_scores():\n",
    "    \"\"\"\n",
    "    Check all JSON files for any self_grade_score that exceeds the max score for that question\n",
    "    or is below 0 (minimum score)\n",
    "    \"\"\"\n",
    "    issues_found = []\n",
    "    \n",
    "    print(\"Checking for invalid scores (> max_score or < 0)...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for method in methods:\n",
    "        method_dir = f\"results/{method}\"\n",
    "        \n",
    "        if os.path.exists(method_dir):\n",
    "            json_files = glob.glob(os.path.join(method_dir, \"evaluated_results_*.json\"))\n",
    "            \n",
    "            for filepath in json_files:\n",
    "                try:\n",
    "                    with open(filepath, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                    \n",
    "                    # Extract model name from filename\n",
    "                    filename = os.path.basename(filepath)\n",
    "                    model_name = filename.replace('evaluated_results_', '').replace('.json', '').split('__')[0]\n",
    "                    \n",
    "                    # Check each question in the file\n",
    "                    for idx, item in enumerate(data):\n",
    "                        if 'self_grade_score' in item and item['self_grade_score'] is not None:\n",
    "                            score = item['self_grade_score']\n",
    "                            \n",
    "                            # Get the max score for this question (assuming same order)\n",
    "                            if idx < len(max_scores_per_question):\n",
    "                                max_score = max_scores_per_question[idx]\n",
    "                            else:\n",
    "                                max_score = 7  # Fallback if index out of range\n",
    "                                print(f\"‚ö†Ô∏è  Question index {idx} out of range, using default max_score=7\")\n",
    "                            \n",
    "                            # Check for invalid scores\n",
    "                            is_invalid = False\n",
    "                            issue_type = \"\"\n",
    "                            \n",
    "                            if score > max_score:\n",
    "                                is_invalid = True\n",
    "                                issue_type = f\"OVER MAX (score: {score}, max: {max_score})\"\n",
    "                            elif score < 0:\n",
    "                                is_invalid = True\n",
    "                                issue_type = f\"BELOW MIN (score: {score}, min: 0)\"\n",
    "                            \n",
    "                            if is_invalid:\n",
    "                                issue = {\n",
    "                                    'method': method,\n",
    "                                    'model': model_name,\n",
    "                                    'question_index': idx,\n",
    "                                    'score': score,\n",
    "                                    'max_score': max_score,\n",
    "                                    'issue_type': issue_type,\n",
    "                                    'file': filepath\n",
    "                                }\n",
    "                                \n",
    "                                # Try to get question text for better identification\n",
    "                                question_text = item.get('question', f\"Question_{idx}\")\n",
    "                                question_preview = question_text[:100] + \"...\" if len(question_text) > 100 else question_text\n",
    "                                issue['question_preview'] = question_preview\n",
    "                                \n",
    "                                issues_found.append(issue)\n",
    "                                \n",
    "                                print(f\"üö® ISSUE FOUND:\")\n",
    "                                print(f\"   Method: {method}\")\n",
    "                                print(f\"   Model: {model_name}\")\n",
    "                                print(f\"   Question Index: {idx}\")\n",
    "                                print(f\"   Issue: {issue_type}\")\n",
    "                                print(f\"   Question: {question_preview}\")\n",
    "                                print(f\"   File: {os.path.basename(filepath)}\")\n",
    "                                print(\"-\" * 40)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error checking {filepath}: {e}\")\n",
    "    \n",
    "    # Summary\n",
    "    if issues_found:\n",
    "        print(f\"\\n‚ö†Ô∏è  TOTAL ISSUES FOUND: {len(issues_found)}\")\n",
    "        \n",
    "        # Count by issue type\n",
    "        over_max_count = sum(1 for issue in issues_found if \"OVER MAX\" in issue['issue_type'])\n",
    "        below_min_count = sum(1 for issue in issues_found if \"BELOW MIN\" in issue['issue_type'])\n",
    "        \n",
    "        print(f\"   - Scores over maximum: {over_max_count}\")\n",
    "        print(f\"   - Scores below minimum (0): {below_min_count}\")\n",
    "        \n",
    "        print(\"\\nSummary by Model:\")\n",
    "        \n",
    "        # Group issues by model\n",
    "        from collections import defaultdict\n",
    "        issues_by_model = defaultdict(list)\n",
    "        for issue in issues_found:\n",
    "            issues_by_model[issue['model']].append(issue)\n",
    "        \n",
    "        for model, model_issues in issues_by_model.items():\n",
    "            print(f\"  {model}: {len(model_issues)} issues\")\n",
    "            for issue in model_issues:\n",
    "                print(f\"    - {issue['method']}, Q{issue['question_index']}: {issue['issue_type']}\")\n",
    "        \n",
    "        print(\"\\nSummary by Method:\")\n",
    "        issues_by_method = defaultdict(list)\n",
    "        for issue in issues_found:\n",
    "            issues_by_method[issue['method']].append(issue)\n",
    "        \n",
    "        for method, method_issues in issues_by_method.items():\n",
    "            print(f\"  {method}: {len(method_issues)} issues\")\n",
    "        \n",
    "        # Create a DataFrame for easier analysis\n",
    "        issues_df = pd.DataFrame(issues_found)\n",
    "        print(f\"\\nSaving issues to 'scoring_issues.csv'\")\n",
    "        issues_df.to_csv('scoring_issues.csv', index=False)\n",
    "        \n",
    "        return issues_found\n",
    "    else:\n",
    "        print(\"‚úÖ No invalid scores found. All scores are within valid ranges!\")\n",
    "        return []\n",
    "\n",
    "# Run the validation check\n",
    "issues = check_invalid_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8ff77620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing default_essay: Found 25 files\n",
      "  codestral-latest-official: 65/149 = 0.4362 (43 questions)\n",
      "  gpt-4o: 97/149 = 0.6510 (43 questions)\n",
      "  grok-3: 100/149 = 0.6711 (43 questions)\n",
      "  groq-llama3.3-70b: 89/149 = 0.5973 (43 questions)\n",
      "  claude-sonnet-4: 95/149 = 0.6376 (43 questions)\n",
      "  claude-3.7-sonnet: 91/149 = 0.6107 (43 questions)\n",
      "  groq-llama-guard-4: 0/149 = 0.0000 (43 questions)\n",
      "  claude-3.5-haiku: 79/149 = 0.5302 (43 questions)\n",
      "  gemini-2.5-pro: 100/149 = 0.6711 (43 questions)\n",
      "  groq-llama-4-scout: 83/149 = 0.5570 (43 questions)\n",
      "  o4-mini: 117/149 = 0.7852 (43 questions)\n",
      "  gemini-2.5-flash: 100/149 = 0.6711 (43 questions)\n",
      "  grok-3-mini-beta-high-effort: 91/149 = 0.6107 (41 questions)\n",
      "  claude-opus-4: 89/149 = 0.5973 (43 questions)\n",
      "  mistral-large-official: 78/149 = 0.5235 (43 questions)\n",
      "  gpt-4.1-mini: 83/149 = 0.5570 (43 questions)\n",
      "  groq-llama-4-maverick: 103/149 = 0.6913 (43 questions)\n",
      "  deepseek-r1: 93/149 = 0.6242 (43 questions)\n",
      "  gpt-4.1: 81/149 = 0.5436 (43 questions)\n",
      "  grok-3-mini-beta-low-effort: 101/149 = 0.6779 (43 questions)\n",
      "  gpt-4.1-nano: 64/149 = 0.4295 (43 questions)\n",
      "  o3-mini: 106/149 = 0.7114 (43 questions)\n",
      "  palmyra-fin-default: 87/149 = 0.5839 (43 questions)\n",
      "  claude-3.5-sonnet: 85/149 = 0.5705 (43 questions)\n",
      "  groq-llama3.1-8b-instant: 61/149 = 0.4094 (43 questions)\n",
      "Processing self_consistency_essay_n3: Found 25 files\n",
      "  claude-3.7-sonnet: 101/149 = 0.6779 (43 questions)\n",
      "  grok-3-mini-beta-low-effort: 104/149 = 0.6980 (43 questions)\n",
      "  gemini-2.5-flash: 121/149 = 0.8121 (43 questions)\n",
      "  groq-llama3.1-8b-instant: 47/149 = 0.3154 (43 questions)\n",
      "  mistral-large-official: 84/149 = 0.5638 (43 questions)\n",
      "  palmyra-fin-default: 75/149 = 0.5034 (43 questions)\n",
      "  o3-mini: 108/149 = 0.7248 (43 questions)\n",
      "  o4-mini: 124/149 = 0.8322 (43 questions)\n",
      "  grok-3: 109/149 = 0.7315 (43 questions)\n",
      "  gpt-4.1-nano: 78/149 = 0.5235 (43 questions)\n",
      "  deepseek-r1: 94/149 = 0.6309 (43 questions)\n",
      "  claude-sonnet-4: 107/149 = 0.7181 (43 questions)\n",
      "  gemini-2.5-pro: 112/149 = 0.7517 (43 questions)\n",
      "  gpt-4.1-mini: 90/149 = 0.6040 (43 questions)\n",
      "  codestral-latest-official: 51/149 = 0.3423 (43 questions)\n",
      "  gpt-4o: 91/149 = 0.6107 (43 questions)\n",
      "  grok-3-mini-beta-high-effort: 104/149 = 0.6980 (43 questions)\n",
      "  gpt-4.1: 81/149 = 0.5436 (43 questions)\n",
      "  groq-llama3.3-70b: 77/149 = 0.5168 (43 questions)\n",
      "  groq-llama-4-scout: 83/149 = 0.5570 (43 questions)\n",
      "  claude-opus-4: 114/149 = 0.7651 (43 questions)\n",
      "  claude-3.5-haiku: 91/149 = 0.6107 (43 questions)\n",
      "  groq-llama-guard-4: 0/149 = 0.0000 (43 questions)\n",
      "  claude-3.5-sonnet: 86/149 = 0.5772 (43 questions)\n",
      "  groq-llama-4-maverick: 86/149 = 0.5772 (43 questions)\n",
      "Processing self_consistency_essay_n5: Found 25 files\n",
      "  grok-3: 113/149 = 0.7584 (43 questions)\n",
      "  gpt-4.1-nano: 74/149 = 0.4966 (43 questions)\n",
      "  o3-mini: 110/149 = 0.7383 (43 questions)\n",
      "  o4-mini: 119/149 = 0.7987 (43 questions)\n",
      "  gemini-2.5-pro: 105/149 = 0.7047 (43 questions)\n",
      "  gpt-4.1-mini: 92/149 = 0.6174 (43 questions)\n",
      "  deepseek-r1: 86/149 = 0.5772 (43 questions)\n",
      "  claude-sonnet-4: 104/149 = 0.6980 (43 questions)\n",
      "  claude-3.7-sonnet: 91/149 = 0.6107 (43 questions)\n",
      "  palmyra-fin-default: 74/149 = 0.4966 (43 questions)\n",
      "  grok-3-mini-beta-low-effort: 100/149 = 0.6711 (43 questions)\n",
      "  gemini-2.5-flash: 112/149 = 0.7517 (43 questions)\n",
      "  groq-llama3.1-8b-instant: 69/149 = 0.4631 (43 questions)\n",
      "  mistral-large-official: 90/149 = 0.6040 (43 questions)\n",
      "  claude-opus-4: 113/149 = 0.7584 (43 questions)\n",
      "  claude-3.5-haiku: 76/149 = 0.5101 (43 questions)\n",
      "  groq-llama-4-scout: 84/149 = 0.5638 (43 questions)\n",
      "  claude-3.5-sonnet: 83/149 = 0.5570 (43 questions)\n",
      "  groq-llama-4-maverick: 97/149 = 0.6510 (43 questions)\n",
      "  groq-llama-guard-4: 0/149 = 0.0000 (43 questions)\n",
      "  gpt-4o: 96/149 = 0.6443 (43 questions)\n",
      "  codestral-latest-official: 56/149 = 0.3758 (43 questions)\n",
      "  gpt-4.1: 86/149 = 0.5772 (43 questions)\n",
      "  groq-llama3.3-70b: 80/149 = 0.5369 (43 questions)\n",
      "  grok-3-mini-beta-high-effort: 102/149 = 0.6846 (43 questions)\n",
      "Processing self_discover_essay: Found 25 files\n",
      "  claude-opus-4: 98/149 = 0.6577 (43 questions)\n",
      "  o3-mini: 110/149 = 0.7383 (43 questions)\n",
      "  claude-3.5-haiku: 67/149 = 0.4497 (43 questions)\n",
      "  gpt-4o: 55/149 = 0.3691 (43 questions)\n",
      "  grok-3: 86/149 = 0.5772 (43 questions)\n",
      "  groq-llama-guard-4: 0/149 = 0.0000 (43 questions)\n",
      "  o4-mini: 106/149 = 0.7114 (43 questions)\n",
      "  groq-llama-4-maverick: 78/149 = 0.5235 (43 questions)\n",
      "  claude-3.7-sonnet: 91/149 = 0.6107 (43 questions)\n",
      "  grok-3-mini-beta-low-effort: 80/149 = 0.5369 (43 questions)\n",
      "  palmyra-fin-default: 49/149 = 0.3289 (43 questions)\n",
      "  groq-llama-4-scout: 63/149 = 0.4228 (43 questions)\n",
      "  gpt-4.1: 68/149 = 0.4564 (43 questions)\n",
      "  claude-sonnet-4: 86/149 = 0.5772 (43 questions)\n",
      "  grok-3-mini-beta-high-effort: 69/149 = 0.4631 (42 questions)\n",
      "  gpt-4.1-mini: 65/149 = 0.4362 (43 questions)\n",
      "  mistral-large-official: 40/149 = 0.2685 (43 questions)\n",
      "  groq-llama3.3-70b: 54/149 = 0.3624 (43 questions)\n",
      "  gemini-2.5-flash: 102/149 = 0.6846 (43 questions)\n",
      "  gemini-2.5-pro: 92/149 = 0.6174 (43 questions)\n",
      "  claude-3.5-sonnet: 83/149 = 0.5570 (43 questions)\n",
      "  deepseek-r1: 80/149 = 0.5369 (43 questions)\n",
      "  codestral-latest-official: 39/149 = 0.2617 (43 questions)\n",
      "  groq-llama3.1-8b-instant: 30/149 = 0.2013 (43 questions)\n",
      "  gpt-4.1-nano: 40/149 = 0.2685 (43 questions)\n"
     ]
    }
   ],
   "source": [
    "# Process each method directory\n",
    "for method in methods:\n",
    "    method_dir = f\"results/{method}\"\n",
    "    \n",
    "    if os.path.exists(method_dir):\n",
    "        # Find all JSON files in the directory\n",
    "        json_files = glob.glob(os.path.join(method_dir, \"evaluated_results_*.json\"))\n",
    "        \n",
    "        print(f\"Processing {method}: Found {len(json_files)} files\")\n",
    "        \n",
    "        for filepath in json_files:\n",
    "            model_name, total_score, num_questions, normalized_score = process_json_file(filepath)\n",
    "            \n",
    "            if model_name:\n",
    "                # Initialize model entry if not exists\n",
    "                if model_name not in results_data:\n",
    "                    results_data[model_name] = {}\n",
    "                \n",
    "                # Store the normalized score for this method\n",
    "                results_data[model_name][method] = normalized_score\n",
    "                \n",
    "                print(f\"  {model_name}: {total_score}/{total_possible_points} = {normalized_score:.4f} ({num_questions} questions)\")\n",
    "    else:\n",
    "        print(f\"Directory {method_dir} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "39f5fd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results ordered by default_essay:\n",
      "================================================================================\n",
      "                              default_essay\n",
      "o4-mini                              0.7852\n",
      "o3-mini                              0.7114\n",
      "groq-llama-4-maverick                0.6913\n",
      "grok-3-mini-beta-low-effort          0.6779\n",
      "grok-3                               0.6711\n",
      "gemini-2.5-pro                       0.6711\n",
      "gemini-2.5-flash                     0.6711\n",
      "gpt-4o                               0.6510\n",
      "claude-sonnet-4                      0.6376\n",
      "deepseek-r1                          0.6242\n",
      "grok-3-mini-beta-high-effort         0.6107\n",
      "claude-3.7-sonnet                    0.6107\n",
      "claude-opus-4                        0.5973\n",
      "groq-llama3.3-70b                    0.5973\n",
      "palmyra-fin-default                  0.5839\n",
      "claude-3.5-sonnet                    0.5705\n",
      "groq-llama-4-scout                   0.5570\n",
      "gpt-4.1-mini                         0.5570\n",
      "gpt-4.1                              0.5436\n",
      "claude-3.5-haiku                     0.5302\n",
      "mistral-large-official               0.5235\n",
      "codestral-latest-official            0.4362\n",
      "gpt-4.1-nano                         0.4295\n",
      "groq-llama3.1-8b-instant             0.4094\n",
      "groq-llama-guard-4                   0.0000\n",
      "\n",
      "Results ordered by self_consistency_essay_n3:\n",
      "================================================================================\n",
      "                              self_consistency_essay_n3\n",
      "o4-mini                                          0.8322\n",
      "gemini-2.5-flash                                 0.8121\n",
      "claude-opus-4                                    0.7651\n",
      "gemini-2.5-pro                                   0.7517\n",
      "grok-3                                           0.7315\n",
      "o3-mini                                          0.7248\n",
      "claude-sonnet-4                                  0.7181\n",
      "grok-3-mini-beta-low-effort                      0.6980\n",
      "grok-3-mini-beta-high-effort                     0.6980\n",
      "claude-3.7-sonnet                                0.6779\n",
      "deepseek-r1                                      0.6309\n",
      "claude-3.5-haiku                                 0.6107\n",
      "gpt-4o                                           0.6107\n",
      "gpt-4.1-mini                                     0.6040\n",
      "claude-3.5-sonnet                                0.5772\n",
      "groq-llama-4-maverick                            0.5772\n",
      "mistral-large-official                           0.5638\n",
      "groq-llama-4-scout                               0.5570\n",
      "gpt-4.1                                          0.5436\n",
      "gpt-4.1-nano                                     0.5235\n",
      "groq-llama3.3-70b                                0.5168\n",
      "palmyra-fin-default                              0.5034\n",
      "codestral-latest-official                        0.3423\n",
      "groq-llama3.1-8b-instant                         0.3154\n",
      "groq-llama-guard-4                               0.0000\n",
      "\n",
      "Results ordered by self_consistency_essay_n5:\n",
      "================================================================================\n",
      "                              self_consistency_essay_n5\n",
      "o4-mini                                          0.7987\n",
      "grok-3                                           0.7584\n",
      "claude-opus-4                                    0.7584\n",
      "gemini-2.5-flash                                 0.7517\n",
      "o3-mini                                          0.7383\n",
      "gemini-2.5-pro                                   0.7047\n",
      "claude-sonnet-4                                  0.6980\n",
      "grok-3-mini-beta-high-effort                     0.6846\n",
      "grok-3-mini-beta-low-effort                      0.6711\n",
      "groq-llama-4-maverick                            0.6510\n",
      "gpt-4o                                           0.6443\n",
      "gpt-4.1-mini                                     0.6174\n",
      "claude-3.7-sonnet                                0.6107\n",
      "mistral-large-official                           0.6040\n",
      "deepseek-r1                                      0.5772\n",
      "gpt-4.1                                          0.5772\n",
      "groq-llama-4-scout                               0.5638\n",
      "claude-3.5-sonnet                                0.5570\n",
      "groq-llama3.3-70b                                0.5369\n",
      "claude-3.5-haiku                                 0.5101\n",
      "palmyra-fin-default                              0.4966\n",
      "gpt-4.1-nano                                     0.4966\n",
      "groq-llama3.1-8b-instant                         0.4631\n",
      "codestral-latest-official                        0.3758\n",
      "groq-llama-guard-4                               0.0000\n",
      "\n",
      "Results ordered by self_discover_essay:\n",
      "================================================================================\n",
      "                              self_discover_essay\n",
      "o3-mini                                    0.7383\n",
      "o4-mini                                    0.7114\n",
      "gemini-2.5-flash                           0.6846\n",
      "claude-opus-4                              0.6577\n",
      "gemini-2.5-pro                             0.6174\n",
      "claude-3.7-sonnet                          0.6107\n",
      "grok-3                                     0.5772\n",
      "claude-sonnet-4                            0.5772\n",
      "claude-3.5-sonnet                          0.5570\n",
      "deepseek-r1                                0.5369\n",
      "grok-3-mini-beta-low-effort                0.5369\n",
      "groq-llama-4-maverick                      0.5235\n",
      "grok-3-mini-beta-high-effort               0.4631\n",
      "gpt-4.1                                    0.4564\n",
      "claude-3.5-haiku                           0.4497\n",
      "gpt-4.1-mini                               0.4362\n",
      "groq-llama-4-scout                         0.4228\n",
      "gpt-4o                                     0.3691\n",
      "groq-llama3.3-70b                          0.3624\n",
      "palmyra-fin-default                        0.3289\n",
      "mistral-large-official                     0.2685\n",
      "gpt-4.1-nano                               0.2685\n",
      "codestral-latest-official                  0.2617\n",
      "groq-llama3.1-8b-instant                   0.2013\n",
      "groq-llama-guard-4                         0.0000\n"
     ]
    }
   ],
   "source": [
    "# Convert results_data ‚Üí DataFrame\n",
    "df = pd.DataFrame.from_dict(results_data, orient=\"index\")\n",
    "\n",
    "# Keep columns in the desired order\n",
    "df = df.reindex(columns=methods)\n",
    "\n",
    "# Replace missing scores with 0\n",
    "df = df.fillna(0)\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# One table per method, ordered by value\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "for method in methods:\n",
    "    ordered_tbl = (\n",
    "        df[[method]]             # keep only that method‚Äôs column\n",
    "        .sort_values(\n",
    "            by=method,\n",
    "            ascending=False      # highest score at the top\n",
    "        )\n",
    "        .round(4)\n",
    "    )\n",
    "\n",
    "    print(f\"\\nResults ordered by {method}:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(ordered_tbl.to_string())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

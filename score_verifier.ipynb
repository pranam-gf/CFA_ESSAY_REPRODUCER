{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2924bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the methods/processes\n",
    "methods = ['default_essay']\n",
    "\n",
    "# Create a dictionary to store all results\n",
    "results_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a9984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_file(filepath):\n",
    "    \"\"\"\n",
    "    Process a single JSON file and calculate the aggregate score\n",
    "    Returns: (model_name, total_score, num_questions, normalized_score)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract model name from filename\n",
    "        filename = os.path.basename(filepath)\n",
    "        # Remove 'evaluated_results_' prefix and '.json' suffix, then split by '__'\n",
    "        model_name = filename.replace('evaluated_results_', '').replace('.json', '').split('__')[0]\n",
    "        \n",
    "        # Calculate total self_grade_score\n",
    "        total_score = 0\n",
    "        num_questions = 0\n",
    "        \n",
    "        for item in data:\n",
    "            if 'self_grade_score' in item and item['self_grade_score'] is not None:\n",
    "                total_score += item['self_grade_score']\n",
    "                num_questions += 1\n",
    "        \n",
    "        # Calculate normalized score (total achieved / total possible)\n",
    "        # Total possible = num_questions * 7\n",
    "        normalized_score = total_score / (num_questions * 7) if num_questions > 0 else 0\n",
    "        \n",
    "        return model_name, total_score, num_questions, normalized_score\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filepath}: {e}\")\n",
    "        return None, 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a5aa5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for scores over 7...\n",
      "============================================================\n",
      "‚úÖ No scores over 7 found. All data looks good!\n"
     ]
    }
   ],
   "source": [
    "def check_scores_over_7():\n",
    "    \"\"\"\n",
    "    Check all JSON files for any self_grade_score over 7 and flag them\n",
    "    \"\"\"\n",
    "    issues_found = []\n",
    "    \n",
    "    print(\"Checking for scores over 7...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for method in methods:\n",
    "        method_dir = f\"results/{method}\"\n",
    "        \n",
    "        if os.path.exists(method_dir):\n",
    "            json_files = glob.glob(os.path.join(method_dir, \"evaluated_results_*.json\"))\n",
    "            \n",
    "            for filepath in json_files:\n",
    "                try:\n",
    "                    with open(filepath, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                    \n",
    "                    # Extract model name from filename\n",
    "                    filename = os.path.basename(filepath)\n",
    "                    model_name = filename.replace('evaluated_results_', '').replace('.json', '').split('__')[0]\n",
    "                    \n",
    "                    # Check each question in the file\n",
    "                    for idx, item in enumerate(data):\n",
    "                        if 'self_grade_score' in item and item['self_grade_score'] is not None:\n",
    "                            score = item['self_grade_score']\n",
    "                            \n",
    "                            if score > 7:\n",
    "                                issue = {\n",
    "                                    'method': method,\n",
    "                                    'model': model_name,\n",
    "                                    'question_index': idx,\n",
    "                                    'score': score,\n",
    "                                    'file': filepath\n",
    "                                }\n",
    "                                \n",
    "                                # Try to get question text or ID for better identification\n",
    "                                question_id = item.get('question_id', item.get('id', f\"Question_{idx}\"))\n",
    "                                issue['question_id'] = question_id\n",
    "                                \n",
    "                                issues_found.append(issue)\n",
    "                                \n",
    "                                print(f\"üö® ISSUE FOUND:\")\n",
    "                                print(f\"   Method: {method}\")\n",
    "                                print(f\"   Model: {model_name}\")\n",
    "                                print(f\"   Question ID: {question_id}\")\n",
    "                                print(f\"   Question Index: {idx}\")\n",
    "                                print(f\"   Score: {score} (over 7!)\")\n",
    "                                print(f\"   File: {os.path.basename(filepath)}\")\n",
    "                                print(\"-\" * 40)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error checking {filepath}: {e}\")\n",
    "    \n",
    "    # Summary\n",
    "    if issues_found:\n",
    "        print(f\"\\n‚ö†Ô∏è  TOTAL ISSUES FOUND: {len(issues_found)}\")\n",
    "        print(\"\\nSummary by Model:\")\n",
    "        \n",
    "        # Group issues by model\n",
    "        from collections import defaultdict\n",
    "        issues_by_model = defaultdict(list)\n",
    "        for issue in issues_found:\n",
    "            issues_by_model[issue['model']].append(issue)\n",
    "        \n",
    "        for model, model_issues in issues_by_model.items():\n",
    "            print(f\"  {model}: {len(model_issues)} issues\")\n",
    "            for issue in model_issues:\n",
    "                print(f\"    - {issue['method']}, Q{issue['question_index']}: {issue['score']}\")\n",
    "        \n",
    "        # Create a DataFrame for easier analysis\n",
    "        issues_df = pd.DataFrame(issues_found)\n",
    "        print(f\"\\nSaving issues to 'scoring_issues.csv'\")\n",
    "        issues_df.to_csv('scoring_issues.csv', index=False)\n",
    "        \n",
    "        return issues_found\n",
    "    else:\n",
    "        print(\"‚úÖ No scores over 7 found. All data looks good!\")\n",
    "        return []\n",
    "\n",
    "# Run the validation check\n",
    "issues = check_scores_over_7()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ff77620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing default_essay: Found 25 files\n"
     ]
    }
   ],
   "source": [
    "# Process each method directory\n",
    "for method in methods:\n",
    "    method_dir = f\"results/{method}\"\n",
    "    \n",
    "    if os.path.exists(method_dir):\n",
    "        # Find all JSON files in the directory\n",
    "        json_files = glob.glob(os.path.join(method_dir, \"evaluated_results_*.json\"))\n",
    "        \n",
    "        print(f\"Processing {method}: Found {len(json_files)} files\")\n",
    "        \n",
    "        for filepath in json_files:\n",
    "            model_name, total_score, num_questions, normalized_score = process_json_file(filepath)\n",
    "            \n",
    "            if model_name:\n",
    "                # Initialize model entry if not exists\n",
    "                if model_name not in results_data:\n",
    "                    results_data[model_name] = {}\n",
    "                \n",
    "                # Store the normalized score for this method\n",
    "                results_data[model_name][method] = normalized_score\n",
    "                \n",
    "                # print(f\"  {model_name}: {total_score}/{num_questions*7} = {normalized_score:.4f}\")\n",
    "    else:\n",
    "        print(f\"Directory {method_dir} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39f5fd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results Table:\n",
      "================================================================================\n",
      "                              default_essay\n",
      "claude-3.5-haiku                     0.4086\n",
      "claude-3.5-sonnet                    0.4319\n",
      "claude-3.7-sonnet                    0.4286\n",
      "claude-opus-4                        0.3821\n",
      "claude-sonnet-4                      0.4053\n",
      "codestral-latest-official            0.3156\n",
      "deepseek-r1                          0.4518\n",
      "gemini-2.5-flash                     0.4452\n",
      "gemini-2.5-pro                       0.4452\n",
      "gpt-4.1                              0.3123\n",
      "gpt-4.1-mini                         0.3588\n",
      "gpt-4.1-nano                         0.3123\n",
      "gpt-4o                               0.4120\n",
      "grok-3                               0.4319\n",
      "grok-3-mini-beta-high-effort         0.4530\n",
      "grok-3-mini-beta-low-effort          0.4518\n",
      "groq-llama-4-maverick                0.4618\n",
      "groq-llama-4-scout                   0.4053\n",
      "groq-llama-guard-4                   0.0000\n",
      "groq-llama3.1-8b-instant             0.3322\n",
      "groq-llama3.3-70b                    0.3953\n",
      "mistral-large-official               0.3555\n",
      "o3-mini                              0.4684\n",
      "o4-mini                              0.4551\n",
      "palmyra-fin-default                  0.3854\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame.from_dict(results_data, orient='index')\n",
    "\n",
    "# Reorder columns to match the specified order\n",
    "df = df.reindex(columns=methods)\n",
    "\n",
    "# Fill NaN values with 0 (in case some models don't have results for all methods)\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Sort by model name for better readability\n",
    "df = df.sort_index()\n",
    "\n",
    "print(\"Final Results Table:\")\n",
    "print(\"=\" * 80)\n",
    "print(df.round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

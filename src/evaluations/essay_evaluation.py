"""
Evaluation metrics for essay-style answers.
"""
import logging
import json 
import copy 
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from typing import Dict, Any, List, Optional
from rouge_score import rouge_scorer
from ..llm_clients import get_llm_response 
from ..prompts.grading_prompts import get_full_structured_grading_prompt 

logger = logging.getLogger(__name__)

def calculate_cosine_similarity(generated_answer: str, reference_answer: str) -> Optional[float]:
    """
    Calculates the cosine similarity between a generated essay and a reference essay.

    Args:
        generated_answer: The essay answer generated by the LLM.
        reference_answer: The ground truth or model explanation.

    Returns:
        The cosine similarity score (float) between 0 and 1, or None if an error occurs
        or inputs are invalid.
    """
    if not generated_answer or not reference_answer:
        logger.warning("Cosine similarity calculation skipped: Empty generated or reference answer.")
        return None
    
    try:
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform([generated_answer, reference_answer])
        similarity_score = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        return float(similarity_score)
    except Exception as e:
        logger.error(f"Error calculating cosine similarity: {e}", exc_info=True)
        return None

def calculate_rouge_l_score(generated_answer: str, reference_answer: str) -> Optional[Dict[str, float]]:
    """
    Calculates ROUGE-L (precision, recall, f1) scores between a generated essay and a reference essay.

    Args:
        generated_answer: The essay answer generated by the LLM.
        reference_answer: The ground truth or model explanation.

    Returns:
        A dictionary with 'precision', 'recall', 'f1measure' for ROUGE-L, or None if an error occurs.
    """
    if not generated_answer or not reference_answer:
        logger.warning("ROUGE-L calculation skipped: Empty generated or reference answer.")
        return None
    try:
        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
        scores = scorer.score(reference_answer, generated_answer) 
        rouge_l_scores = {
            "precision": scores['rougeL'].precision,
            "recall": scores['rougeL'].recall,
            "f1measure": scores['rougeL'].fmeasure
        }
        return rouge_l_scores
    except Exception as e:
        logger.error(f"Error calculating ROUGE-L score: {e}", exc_info=True)
        return None

def llm_self_grade(question_data: Dict[str, Any], llm_answer: str, model_config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Performs LLM self-grading of a generated essay answer.

    Args:
        question_data: The original parsed question data (containing 'question', 'explanation', etc.).
        llm_answer: The answer generated by the primary LLM.
        model_config: Configuration for the LLM to be used for grading.
                      (For self-grading, this is the same model that generated the answer).

    Returns:
        A dictionary containing the grading score and justification 
        (e.g., {"score": 8, "justification": "...", "error": None}).
        Returns error info if grading fails.
    """
    logger.info(f"Performing LLM self-grading for model {model_config.get('config_id')}")

    original_question = question_data.get('question')
    reference_answer = question_data.get('explanation')

    if not all([original_question, reference_answer, llm_answer]):
        logger.warning("LLM self-grading skipped: Missing original question, reference answer, or LLM answer.")
        return {"score": None, "justification": "Grading skipped due to missing inputs.", "error": "Missing inputs"}

    grading_prompt = get_full_structured_grading_prompt(
        question_text=original_question,
        reference_answer_text=reference_answer,
        generated_answer_text=llm_answer
    )    
    grading_model_config = copy.deepcopy(model_config)
    try:  
        grading_api_response = get_llm_response(
            prompt=grading_prompt,
            model_config=grading_model_config,
            is_json_response_expected=True
        )

        if grading_api_response is None:
            logger.error("LLM self-grading API call returned None.")
            return {"score": None, "justification": "Grading API call failed (returned None).", "error": "API call returned None"}

        if grading_api_response.get('error_message'):
            error_msg = grading_api_response['error_message']
            logger.error(f"LLM self-grading API call failed: {error_msg}")
            return {"score": None, "justification": f"Grading API call error: {error_msg}", "error": error_msg}

        parsed_grade_data = grading_api_response.get('response_content')

        if not isinstance(parsed_grade_data, dict):
            logger.error(f"LLM self-grading response was not a dictionary: {parsed_grade_data}")
            raw_text = grading_api_response.get('raw_response_text', 'N/A')
            return {"score": None, "justification": f"Grading response parsing failed (not a dict). Raw: {raw_text[:100]}", "error": "Invalid response format (not a dict)"}

        score = parsed_grade_data.get('score')
        justification = parsed_grade_data.get('justification')

        if score is None or justification is None:
            logger.error(f"LLM self-grading response missing 'score' or 'justification'. Data: {parsed_grade_data}")
            return {"score": None, "justification": "Grading response incomplete (missing score/justification).", "error": "Incomplete response"}
        
        if not isinstance(score, int) or not (1 <= score <= 10):
            logger.error(f"LLM self-grading score '{score}' is not an integer between 1 and 10.")
            return {"score": None, "justification": justification, "error": f"Invalid score value: {score}"}

        logger.info(f"LLM self-grade successful for {model_config.get('config_id')}: Score {score}/10")
        return {"score": score, "justification": justification, "error": None}

    except Exception as e:
        logger.error(f"Exception during LLM self-grading: {e}", exc_info=True)
        return {"score": None, "justification": f"Exception during grading: {str(e)}", "error": str(e)}

def evaluate_essay_answers(results_data: List[Dict[str, Any]], grading_model_config: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
    """
    Evaluates a list of generated essay answers using various metrics.
    Uses cleaned answer for similarity metrics and raw answer for LLM self-grading.

    Args:
        results_data: A list of dictionaries, where each dictionary contains
                      the parsed question data (including 'explanation'), 
                      'raw_llm_answer', and 'cleaned_llm_answer'.
        grading_model_config: Optional model configuration for the LLM self-grading.
                              If None, self-grading might use a default or be skipped.

    Returns:
        The input list with added evaluation metrics.
    """
    evaluated_results = []
    for result in results_data:
        
        cleaned_generated_answer = result.get("cleaned_llm_answer") 
        raw_generated_answer = result.get("raw_llm_answer") 
        reference_answer = result.get("explanation")
        
        if cleaned_generated_answer and reference_answer and not result.get("error"):
            cosine_sim = calculate_cosine_similarity(cleaned_generated_answer, reference_answer)
            result["cosine_similarity"] = cosine_sim
            if cosine_sim is not None:
                 logger.debug(f"Cosine similarity for one item: {cosine_sim:.4f}")
            else:
                 logger.warning("Cosine similarity returned None for an item.")
        else:
            result["cosine_similarity"] = None
            logger.debug("Skipping cosine similarity due to missing cleaned answer, reference, or prior error.")
        
        if cleaned_generated_answer and reference_answer and not result.get("error"):
            rouge_l_scores = calculate_rouge_l_score(cleaned_generated_answer, reference_answer)
            if rouge_l_scores:
                result["rouge_l_precision"] = rouge_l_scores["precision"]
                result["rouge_l_recall"] = rouge_l_scores["recall"]
                result["rouge_l_f1measure"] = rouge_l_scores["f1measure"]
                logger.debug(f"ROUGE-L for one item: P={rouge_l_scores['precision']:.4f}, R={rouge_l_scores['recall']:.4f}, F1={rouge_l_scores['f1measure']:.4f}")
            else:
                result["rouge_l_precision"] = None
                result["rouge_l_recall"] = None
                result["rouge_l_f1measure"] = None
                logger.warning("ROUGE-L calculation returned None for an item.")
        else:
            result["rouge_l_precision"] = None
            result["rouge_l_recall"] = None
            result["rouge_l_f1measure"] = None
            logger.debug("Skipping ROUGE-L due to missing cleaned answer, reference, or prior error.")
        
        if grading_model_config and raw_generated_answer and not result.get("error"):
        
            self_grade_output = llm_self_grade(result, raw_generated_answer, grading_model_config)
            result["self_grade_score"] = self_grade_output.get("score")
            result["self_grade_justification"] = self_grade_output.get("justification")
            if self_grade_output.get("error"):
                result["self_grade_error"] = self_grade_output.get("error")
            logger.debug(f"Self-grade output for one item: Score {result.get('self_grade_score')}")
        else:
            result["self_grade_score"] = None
            result["self_grade_justification"] = "Self-grading skipped (no grading model, no raw answer, or prior error)."
            result["self_grade_error"] = result.get("self_grade_error", "Skipped as per conditions")
            logger.debug("Skipping self-grading for an item.")
            
        evaluated_results.append(result)
    
    return evaluated_results 